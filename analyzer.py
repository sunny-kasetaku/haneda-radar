import re
import datetime
import json
import os
import unicodedata
from config import CONFIG

def run_analyze():
    print("--- KASETACK Analyzer v16.0: å›½å†…ç·šãƒ»å¾¹åº•ç›£æŸ»ç‰ˆ ---")
    if not os.path.exists(CONFIG["DATA_FILE"]):
        print("âŒ ã‚¨ãƒ©ãƒ¼: raw_flight.txt ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        return None

    jst = datetime.timezone(datetime.timedelta(hours=9))
    now = datetime.datetime.now(jst)
    
    with open(CONFIG["DATA_FILE"], "r", encoding="utf-8", errors='ignore') as f:
        raw_html = f.read()

    # 1. å‰å‡¦ç†ï¼šæ­£è¦åŒ–ã¨HTMLã‚¿ã‚°ã®é™¤å»
    content = unicodedata.normalize('NFKC', raw_html)
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚·ãƒ¼ãƒˆã®ä¸­èº«ã‚’ç‰©ç†çš„ã«æ¶ˆå»
    clean_html = re.sub(r'<(style|script)[^>]*>.*?</\1>', ' ', content, flags=re.DOTALL | re.IGNORECASE)
    # ã‚¿ã‚°ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
    text = re.sub(r'<[^>]+>', ' ', clean_html)
    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
    text = re.sub(r'\s+', ' ', text)

    # ğŸ” ã€è¨¼æ‹ é–‹ç¤ºã€‘ãƒ†ã‚­ã‚¹ãƒˆã®å†’é ­1200æ–‡å­—ã‚’ç›´æ¥è¡¨ç¤ºã€‚ã“ã“ã§ã€Œä¾¿åã€ã‚„ã€Œæ™‚åˆ»ã€ãŒè¦‹ãˆã‚‹ã‹ç¢ºèª
    print(f"\n--- ğŸ“‹ ç”Ÿãƒ‡ãƒ¼ã‚¿æ–­ç‰‡ï¼ˆç›®è¦–ç¢ºèªç”¨ï¼‰ ---")
    print(text[300:1500])
    print(f"-----------------------------------\n")

    stands = {"P1": 0, "P2": 0, "P3": 0, "P4": 0, "P5": 0}
    active_rows = []
    found_all_list = [] # çª“ã«é–¢ä¿‚ãªãè¦‹ã¤ã‹ã£ãŸå…¨ãƒ•ãƒ©ã‚¤ãƒˆç”¨

    # 2. èˆªç©ºä¼šç¤¾åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    carrier_map = {
        "JAL": ["JAL", "JL", "æ—¥æœ¬èˆªç©º"],
        "ANA": ["ANA", "NH", "å…¨æ—¥ç©º"],
        "SKY": ["SKY", "BC", "ã‚¹ã‚«ã‚¤ãƒãƒ¼ã‚¯"],
        "ADO": ["ADO", "AIR DO"],
        "SNA": ["SNA", "ã‚½ãƒ©ã‚·ãƒ‰"],
        "SFJ": ["SFJ", "ã‚¹ã‚¿ãƒ¼ãƒ•ãƒ©ã‚¤ãƒ¤ãƒ¼"]
    }

    # 3. è§£æï¼šå…¨ã¦ã®æ™‚åˆ»ã€ŒHH:MMã€ã‚’èµ·ç‚¹ã«å‘¨å›²ã‚’ã‚¹ã‚­ãƒ£ãƒ³
    time_matches = list(re.finditer(r'(\d{1,2}:\d{2})', text))
    print(f"1. ãƒšãƒ¼ã‚¸å†…ã« {len(time_matches)} å€‹ã®æ™‚åˆ»è¡¨è¨˜ã‚’ç¢ºèªã€‚ç²¾æŸ»ã‚’é–‹å§‹...")

    for m in time_matches:
        time_str = m.group(1)
        # æ™‚åˆ»ã®å¾Œæ–¹250æ–‡å­—ã‚’ãƒ‡ãƒ¼ã‚¿å¡Šã¨ã—ã¦åˆ‡ã‚Šå‡ºã—
        chunk = text[m.start() : m.start() + 250].upper()
        
        # èˆªç©ºä¼šç¤¾ã®ç‰¹å®š
        found_c = None
        for code, keywords in carrier_map.items():
            if any(kw in chunk for kw in keywords):
                found_c = code; break
        
        if not found_c: continue # èˆªç©ºä¼šç¤¾ãŒè¦‹ã¤ã‹ã‚‰ãªã„æ™‚åˆ»ã¯ãƒã‚¤ã‚º
        
        # éƒ½å¸‚åã®ç‰¹å®š
        origin = "ä¸æ˜"
        for city in (CONFIG["SOUTH_CITIES"] + CONFIG["NORTH_CITIES"]):
            if city in chunk:
                origin = city; break

        # ã€é‡è¦ã€‘ç›£æŸ»ç”¨ã«å…¨ã¦ã®ç™ºè¦‹ä¾¿ã‚’ãƒªã‚¹ãƒˆã«è“„ç©
        found_all_list.append(f"[{time_str} {found_c} ({origin})]")

        # 4. é€šå¸¸ã®éœ€è¦çª“åˆ¤å®š (T-30 ã€œ T+45)
        h, m_val = map(int, time_str.split(':'))
        f_t = now.replace(hour=h, minute=m_val, second=0, microsecond=0)
        diff = (f_t - now).total_seconds() / 60
        if diff < -720: f_t += datetime.timedelta(days=1); diff += 1440
        elif diff > 720: f_t -= datetime.timedelta(days=1); diff -= 1440

        if CONFIG["WINDOW_PAST"] <= diff <= CONFIG["WINDOW_FUTURE"]:
            active_rows.append({"time": time_str, "flight": found_c, "origin": origin, "pax": 150, "s_key": "P1"})

    # --- ğŸ“œ å¾¹åº•ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ› ---
    print(f"--- ğŸ“Š ç›£æŸ»å ±å‘Šçµæœ ---")
    if found_all_list:
        print(f"âœ… æˆåŠŸ: ãƒšãƒ¼ã‚¸å†…ã‹ã‚‰è¨ˆ {len(found_all_list)} ä»¶ã®ãƒ•ãƒ©ã‚¤ãƒˆè¨˜è¿°ã‚’ç™ºæ˜ã—ã¾ã—ãŸã€‚")
        print(f"æŠ½å‡ºã‚µãƒ³ãƒ—ãƒ«: {', '.join(found_all_list[:10])} ...")
    else:
        print(f"âŒ å¤±æ•—: æ™‚åˆ»ã¯ã‚ã‚Šã¾ã—ãŸãŒã€æœ‰åŠ¹ãªãƒ•ãƒ©ã‚¤ãƒˆè¨˜è¿°ï¼ˆJAL/ANAç­‰ï¼‰ãŒå‘¨å›²ã«ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    print(f"ğŸ¯ éœ€è¦çª“å†…ã®æœ‰åŠ¹ä¾¿: {len(active_rows)} ä¾¿")
    print(f"----------------------")

    result = {
        "stands": stands, "total_pax": len(active_rows)*150, "rows": active_rows, 
        "total_flights_on_page": len(found_all_list), "update_time": now.strftime("%H:%M")
    }
    with open(CONFIG["RESULT_JSON"], "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    return result
