import re
import datetime
import json
import os
from config import CONFIG

def run_analyze():
    print("--- KASETACK Analyzer v5.5: Next.js JSON æ•‘æ¸ˆç‰ˆ ---")
    if not os.path.exists(CONFIG["DATA_FILE"]):
        print("âŒ ã‚¨ãƒ©ãƒ¼: raw_flight.txt ãŒã‚ã‚Šã¾ã›ã‚“")
        return None

    jst = datetime.timezone(datetime.timedelta(hours=9))
    now = datetime.datetime.now(jst)
    
    with open(CONFIG["DATA_FILE"], "r", encoding="utf-8", errors='ignore') as f:
        raw_content = f.read()

    stands = {"P1": 0, "P2": 0, "P3": 0, "P4": 0, "P5": 0}
    flight_rows = []
    
    # æ™‚åˆ»æ¤œç´¢
    time_pattern = r'(\d{1,2})\s*[:ï¼š]\s*(\d{2})'
    time_matches = list(re.finditer(time_pattern, raw_content))
    print(f"1. èª¿æŸ»åœ°ç‚¹: {len(time_matches)}ä»¶ ãƒ’ãƒƒãƒˆ")

    debug_done = False

    for m in time_matches:
        try:
            h_str, m_str = m.groups()
            f_h, f_m = int(h_str), int(m_str)
            f_t = now.replace(hour=f_h % 24, minute=f_m, second=0, microsecond=0)
            diff = (f_t - now).total_seconds() / 60
            
            if not (CONFIG["WINDOW_PAST"] <= diff <= CONFIG["WINDOW_FUTURE"]):
                continue

            # æ¢ç´¢ç¯„å›² (Â±500æ–‡å­—)
            chunk = raw_content[max(0, m.start()-500) : m.end()+500]
            chunk_upper = chunk.upper()
            
            # --- å¼·åˆ¶ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®1ä»¶ã ã‘ä¸­èº«ã‚’éœ²å‡ºã•ã›ã‚‹ ---
            if not debug_done:
                print(f"ğŸ” DEBUG (1st match around {h_str}:{m_str}): {chunk[:300].replace('\\n',' ')}")
                debug_done = True

            # --- ä¾¿åã®æŠ½å‡ºï¼ˆç·©ã‚„ã‹ãªãƒãƒƒãƒãƒ³ã‚°ï¼‰ ---
            carrier = "ä¸æ˜"
            fnum = ""
            carriers = ["JAL", "JL", "ANA", "NH", "BC", "SKY", "ADO", "SNA", "SFJ", "7G", "6J"]
            for c_code in carriers:
                if c_code in chunk_upper:
                    carrier = c_code
                    # ç›´å¾Œã®æ•°å­—ã‚’æ¢ã™
                    fnum_m = re.search(carrier + r'[^0-9]{0,10}(\d{1,4})', chunk_upper)
                    fnum = fnum_m.group(1) if fnum_m else ""
                    break

            # ä¾¿åã™ã‚‰è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if carrier == "ä¸æ˜":
                continue

            # --- å‡ºèº«åœ°ã®æŠ½å‡º ---
            origin = "ä¸æ˜"
            all_cities = CONFIG["SOUTH_CITIES"] + CONFIG["NORTH_CITIES"]
            for city in all_cities:
                if city in chunk:
                    origin = city
                    break
            
            if origin == "ä¸æ˜":
                # JSONå½¢å¼ã‚„ã‚¿ã‚°å½¢å¼ã‹ã‚‰æ—¥æœ¬èªã‚’æŠœã
                org_m = re.search(r'[\">]([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾ ]{2,10})[\"<]', chunk)
                if org_m: origin = org_m.group(1).strip()

            # --- äººæ•°è¨ˆç®— ---
            cap = CONFIG["CAPACITY"]["SMALL"]
            if any(x in chunk_upper for x in ["777", "787", "350", "767", "A330"]):
                cap = CONFIG["CAPACITY"]["BIG"]
            
            pax = int(cap * CONFIG["LOAD_FACTORS"]["NORMAL"])
            
            # --- ä¹—ã‚Šå ´åˆ¤å®š ---
            s_key = "P5" 
            if carrier in ["JAL", "JL"]:
                if origin in CONFIG["NORTH_CITIES"]: s_key = "P2"
                else: s_key = "P1"
            elif carrier in ["NH", "ANA"]: s_key = "P3"
            elif carrier in ["BC", "SKY"]: s_key = "P1"
            elif any(c in carrier for c in ["ADO", "SNA", "SFJ", "7G", "6J"]): s_key = "P4"
            
            flight_rows.append({
                "time": f"{f_h:02d}:{f_m:02d}", "flight": f"{carrier}{fnum}", 
                "origin": origin[:6], "pax": pax, "s_key": s_key
            })

        except: continue

    unique_rows = []
    seen = set()
    for r in flight_rows:
        id_str = f"{r['time']}-{r['flight']}-{r['origin']}"
        if id_str not in seen:
            seen.add(id_str); unique_rows.append(r)

    for r in unique_rows: stands[r['s_key']] += r['pax']
    
    result = {
        "stands": stands, "pool_preds": {k: max(0, 100 - int(v/10)) for k, v in stands.items()},
        "total_pax": sum(stands.values()), "rows": unique_rows, "update_time": now.strftime("%H:%M")
    }
    
    with open(CONFIG["RESULT_JSON"], "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"2. è§£æå®Œäº†ã€‚æœ‰åŠ¹ä¾¿æ•°: {len(unique_rows)} / ç·éœ€è¦: {result['total_pax']}äºº")
    return result
